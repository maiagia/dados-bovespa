# Tech Challenge 3MLET

## Equipe
**Membros:**  
- Kleryton de Souza Maria  
- Lucas Paim de Paula  
- Maiara Giavoni  
- Rafael Tafelli dos Santos 

## **Tech Challenge - Fase 3**

## üé• Demonstra√ß√£o do Projeto  
<video width="100%" controls>
  <source src="https://github.com/maiagia/dados-bovespa/raw/main/Entrega%20Fase%203.mp4" type="video/mp4">
  Seu navegador n√£o suporta a reprodu√ß√£o de v√≠deos.
</video>


Nesta fase, foi desenvolvido um modelo de **Machine Learning** para previs√£o de dados do mercado financeiro, utilizando dados extra√≠dos e armazenados na **Fase 2**. O objetivo foi aprimorar as an√°lises e fornecer uma interface interativa para interpreta√ß√£o dos resultados.

### **Objetivo do Projeto**

Criar um modelo preditivo para identificar tend√™ncias no mercado de a√ß√µes, utilizando t√©cnicas avan√ßadas de Machine Learning. O pipeline completo inclui coleta de dados, processamento, modelagem e visualiza√ß√£o interativa.

### **Atendimento aos Requisitos**

A seguir, indicamos onde cada requisito do **Tech Challenge - Fase 3** foi atendido:

1. **API para coleta de dados em tempo real e armazenamento em Data Lake:**  
   - Reaproveitamos a **API desenvolvida na Fase 2** para coletar dados do mercado financeiro e armazen√°-los no **AWS S3**.  
   - **Mais detalhes:** Se√ß√£o **"Tech Challenge - Fase 2"**.

2. **Constru√ß√£o de um modelo de Machine Learning:**  
   - Desenvolvemos um modelo utilizando **Random Forest Regressor** para prever tend√™ncias do mercado financeiro.  
   - **Mais detalhes:** Se√ß√£o **"Constru√ß√£o do Modelo de Machine Learning"**.

3. **C√≥digo no GitHub com documenta√ß√£o completa:**  
   - Todo o c√≥digo est√° dispon√≠vel no reposit√≥rio, junto com este README e demais documenta√ß√µes.  

4. **Storytelling explicando todas as etapas do projeto em v√≠deo:**  
   - Criamos um v√≠deo explicativo abordando desde a coleta de dados at√© a previs√£o dos resultados.  

5. **Modelo produtivo alimentando uma aplica√ß√£o ou dashboard:**  
   - Implementamos um **dashboard interativo** que apresenta os resultados das previs√µes do modelo, permitindo an√°lise din√¢mica.  
   - **Mais detalhes:** Se√ß√£o **"Dashboard Interativo"**.

### **Principais Entreg√°veis:**

- **Modelo de Machine Learning** baseado em **Random Forest Regressor**.
- **API de coleta de dados**, reaproveitada da **Fase 2**.
- **Armazenamento no AWS S3**, garantindo escalabilidade.
- **Dashboard interativo** para an√°lise das previs√µes.
- **Storytelling do projeto**, documentado em v√≠deo.

### **Constru√ß√£o do Modelo de Machine Learning**

#### **Pr√©-processamento dos Dados:**
- Tratamento de valores nulos e outliers.
- Normaliza√ß√£o e padroniza√ß√£o das vari√°veis.
- Sele√ß√£o das principais caracter√≠sticas.
- Convers√£o de formatos para compatibilidade com o modelo.

#### **Divis√£o dos Dados:**
- Separa√ß√£o em **80% treino** e **20% teste** para garantir uma avalia√ß√£o justa do modelo.

#### **Escolha do Algoritmo:**
- **Random Forest Regressor** foi escolhido por sua robustez, capacidade de generaliza√ß√£o e efici√™ncia na modelagem de s√©ries temporais e dados financeiros.

#### **Treinamento do Modelo:**
- Ajuste de hiperpar√¢metros, incluindo:
  - N√∫mero de trees na Random Forest.
  - Profundidade m√°xima das trees.
  - Crit√©rio de divis√£o das trees.
- Valida√ß√£o cruzada para evitar overfitting e melhorar a generaliza√ß√£o do modelo.
- Ajuste de pesos para lidar com vari√°veis de maior impacto no mercado.

#### **Avalia√ß√£o do Modelo:**
- **MAE (Erro Absoluto M√©dio):** 0.0417
- **MSE (Erro Quadr√°tico M√©dio):** 0.0101
- **RMSE (Raiz do Erro Quadr√°tico M√©dio):** 0.1006

Esses resultados indicam que o modelo apresenta um erro baixo e previs√µes confi√°veis, permitindo uma boa adapta√ß√£o √†s varia√ß√µes do mercado.

#### **Exporta√ß√£o do Modelo:**
- O modelo foi salvo no formato serializado para futuras execu√ß√µes e otimiza√ß√µes.
- Implementa√ß√£o de um pipeline automatizado para reentrenamento conforme novos dados forem incorporados.
- Integra√ß√£o direta com o **dashboard interativo** para an√°lise cont√≠nua dos resultados.

### **Dashboard Interativo**

O c√≥digo do projeto inclui a implementa√ß√£o de um dashboard interativo para visualizar as previs√µes geradas pelo modelo. Esse dashboard permite a an√°lise detalhada das tend√™ncias do mercado financeiro e a compara√ß√£o entre os valores previstos e os valores reais.

---

## **Conclus√£o sobre o Modelo**

O modelo desenvolvido demonstrou **boa capacidade preditiva**, com baixos √≠ndices de erro e uma abordagem escal√°vel para an√°lise financeira. A combina√ß√£o de **Random Forest Regressor** com um pipeline estruturado permitiu previs√µes confi√°veis, ajudando na tomada de decis√£o no mercado financeiro.

Pontos-chave do modelo:
- **Baixo erro m√©dio**, indicando alta precis√£o nas previs√µes.
- **Adapta√ß√£o √†s flutua√ß√µes do mercado**, garantindo previs√µes consistentes.
- **Facilidade de reentrenamento**, permitindo melhoria cont√≠nua com novos dados.
- **Integra√ß√£o com dashboard**, tornando os insights acess√≠veis e f√°ceis de interpretar.

Com essa abordagem, garantimos uma solu√ß√£o **precisa, escal√°vel e eficiente**, agregando valor real √† an√°lise do mercado financeiro.

üìå **Reposit√≥rio:** [GitHub do Projeto](https://github.com/maiagia/dados-bovespa)  


## Tech Challenge - Fase 2
https://github.com/user-attachments/assets/d8f6d0f6-3d8c-4ed4-9d62-667b1078d489

## Descri√ß√£o do Projeto
Este projeto implementa um pipeline de dados completo para extrair, processar e analisar dados do preg√£o da B3 utilizando servi√ßos da AWS, como **S3**, **Glue**, **Lambda** e **Athena**. O objetivo √© automatizar a ingest√£o, transforma√ß√£o e an√°lise de dados hist√≥ricos do mercado de a√ß√µes da B3.
---

## Arquitetura
A arquitetura do pipeline segue o seguinte fluxo:

![401786235-579b8781-1676-4acb-b018-dd955ae0e8aa](https://github.com/user-attachments/assets/f5b55231-d0c7-4a10-be8f-6e64c7d82033)


1. **Scrap de Dados (Lambda)**: A fun√ß√£o Lambda extrai dados do site da B3 e os envia para o bucket S3 de dados brutos.  
2. **Bucket S3 (Dados Brutos)**: Armazena os dados brutos em formato parquet, particionados diariamente.  
3. **Trigger Lambda**: Quando novos arquivos s√£o carregados no S3, uma trigger Lambda inicia o job Glue.  
4. **ETL no Glue**: O job Glue realiza transforma√ß√µes nos dados, como:
   - Agrupamento, sumariza√ß√£o e contagem.
   - Renomea√ß√£o de colunas.
   - C√°lculo da diferen√ßa entre datas.
5. **Bucket S3 (Dados Refinados)**: Os dados transformados s√£o salvos em formato **Parquet**, particionados por data e nome da a√ß√£o.
   * O nome do bucket deve ser √∫nico na aws devido a isso, foi nomeado de refined-bovespa.  
7. **Glue Catalog**: Os metadados dos dados refinados s√£o automaticamente catalogados no Glue, criando uma tabela no banco de dados padr√£o.  
8. **Athena**: Permite consultas SQL interativas e an√°lises dos dados catalogados.
9. **Notebook**: Oferece uma interface interativa para executar consultas SQL diretamente nos dados catalogados no Glue, permitindo an√°lises explorat√≥rias, visualiza√ß√£o de resultados e cria√ß√£o de relat√≥rios de forma pr√°tica, sem necessidade de ferramentas externas. 

---

## Requisitos Atendidos

### 1. **Scrap de Dados do Site da B3**
- A fun√ß√£o Lambda extrai os dados diretamente do site da B3 e os envia ao bucket S3 de dados brutos. `Lambda request-ibov`

### 2. **Ingest√£o de Dados no S3**
- Os dados s√£o armazenados em S3 no formato **Parquet**, organizados em parti√ß√µes di√°rias.

### 3. **Trigger Lambda**
- A Lambda monitora o bucket de dados brutos e aciona o job Glue automaticamente quando novos arquivos s√£o carregados. `Lambda triggerGlueBovespa`

### 4. **Transforma√ß√µes no Glue**
`etl-transformacao-dados-bovespa`.

#### **Agrupamento num√©rico, sumariza√ß√£o e contagem (5A):**
- **N√≥ usado:** `Aggregate_node1736383572625`.
- **Opera√ß√µes realizadas:**
  - **Sumariza√ß√£o:** Soma da coluna `val_participacao_setor`.
  - **Contagem:** Contagem distinta de `cod_empresa`.

#### **Renomea√ß√£o de colunas (5B):**
- **N√≥ usado:** `ChangeSchema_node1736384274380`.
- **Colunas renomeadas:**
  - `nme_acao` ‚Üí `NomeAcao`.
  - `dt_referencia_carteira` ‚Üí `DataReferenciaCarteira`.

#### **C√°lculo com campos de data (5C):**
- **Fun√ß√£o personalizada:** `MyTransform`.
- **Descri√ß√£o:** Calcula a diferen√ßa de dias entre a data atual (`current_date()`) e a coluna `DataReferenciaCarteira`. O resultado √© armazenado em uma nova coluna, `DiferencaDias`.

### 5. **Salvar os Dados Refinados no S3**
- **Local de salvamento:** `s3://refined-bovespa/tb_transformacao_dados_bovespa/`.
- **Parti√ß√µes:** `DataReferenciaCarteira` e `NomeAcao`.

### 6. **Catalogar Dados no Glue Catalog**
- Os dados transformados s√£o automaticamente catalogados no Glue Catalog, permitindo consultas no Athena.

### 7. **Disponibilizar Dados no Athena**
- Como os dados s√£o catalogados automaticamente, eles ficam dispon√≠veis para consultas SQL no Athena.

### 8. **Cria√ß√£o do Notebook no Athena**
- No notebook, inclua o c√≥digo necess√°rio para manipula√ß√£o de dados e an√°lises adicionais.

---

## Etapas do Pipeline

### **Job ETL no Glue**
O job foi desenvolvido no AWS Glue Studio e realiza as seguintes opera√ß√µes:
- **Leitura dos Dados**: Importa os dados brutos do S3 no formato **Parquet**.
- **Transforma√ß√£o dos Dados**:
  - Renomeia colunas para adequa√ß√£o do esquema.
  - Agrega e calcula m√©tricas num√©ricas.
  - Calcula diferen√ßas entre datas.
- **Escrita dos Dados**: Salva os dados transformados em formato **Parquet** no S3, particionados por data e nome da a√ß√£o.
- **Cataloga√ß√£o**: Atualiza automaticamente o Glue Catalog com os metadados dos dados refinados.
Exemplo do c√≥digo do ETL:
```python
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrameCollection
from awsglue.dynamicframe import DynamicFrame
from pyspark.sql import functions as SqlFuncs

# Script generated for node Custom Transform
def MyTransform(glueContext, dfc) -> DynamicFrameCollection:
    # Importando as bibliotecas necess√°rias dentro da fun√ß√£o
    from pyspark.sql.functions import col, datediff, to_date, current_date
    from awsglue.dynamicframe import DynamicFrame

    # Configura o Spark para usar o parser de data legado
    glueContext.spark_session.conf.set("spark.sql.legacy.timeParserPolicy", "LEGACY")

    # Acessa o DynamicFrame espec√≠fico dentro do DynamicFrameCollection
    dynamic_frame = dfc["ChangeSchema_node1736451072089"]

    # Converte o DynamicFrame para DataFrame para usar as fun√ß√µes Spark
    df = dynamic_frame.toDF()

    # Converte as colunas para 'timestamp' (se necess√°rio)
    df = df.withColumn(
        "DataReferenciaCarteira", 
        to_date(col("DataReferenciaCarteira"), "yyyyMMdd")  # Converte 'DataReferenciaCarteira' para tipo Date
    )

    # Calcula a diferen√ßa entre a data atual e a coluna 'DataReferenciaCarteira'
    df = df.withColumn(
        "DiferencaDias", 
        datediff(current_date(), col("DataReferenciaCarteira"))
    )

    # Substitui valores nulos por 0
    df = df.fillna({"DiferencaDias": 0})

    # Reorganiza as colunas para a ordem desejada
    df = df.select(
        "NomeAcao", 
        "DataReferenciaCarteira", 
        "QtdTeoricaTotal", 
        "DiferencaDias", 
        "SomaParticipacaoSetor", 
        "ContagemEmpresas"
    )

    # Converte o DataFrame de volta para DynamicFrame
    transformed_dynamic_frame = DynamicFrame.fromDF(df, glueContext, "transformed_dynamic_frame")

    # Retorna o DynamicFrame dentro de um DynamicFrameCollection
    return DynamicFrameCollection({"transformed_dynamic_frame": transformed_dynamic_frame}, glueContext)
def sparkAggregate(glueContext, parentFrame, groups, aggs, transformation_ctx) -> DynamicFrame:
    aggsFuncs = []
    for column, func in aggs:
        aggsFuncs.append(getattr(SqlFuncs, func)(column))
    result = parentFrame.toDF().groupBy(*groups).agg(*aggsFuncs) if len(groups) > 0 else parentFrame.toDF().agg(*aggsFuncs)
    return DynamicFrame.fromDF(result, glueContext, transformation_ctx)

args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Script generated for node Amazon S3
AmazonS3_node1736304019816 = glueContext.create_dynamic_frame.from_options(format_options={}, connection_type="s3", format="parquet", connection_options={"paths": ["s3://stage-dados-brutos/ano=2025/"], "recurse": True}, transformation_ctx="AmazonS3_node1736304019816")

# Script generated for node Change Schema
ChangeSchema_node1736384274380 = ApplyMapping.apply(frame=AmazonS3_node1736304019816, mappings=[("nme_setor", "string", "nme_setor", "string"), ("cod_empresa", "string", "cod_empresa", "string"), ("nme_acao", "string", "NomeAcao", "string"), ("dsc_tipo", "string", "dsc_tipo", "string"), ("val_participacao_setor", "double", "val_participacao_setor", "double"), ("val_participacao_acumulada_setor", "double", "val_participacao_acumulada_setor", "string"), ("qtd_teorica", "double", "qtd_teorica", "string"), ("dt_referencia_carteira", "bigint", "dt_referencia_carteira", "string"), ("qtd_teorica_total", "double", "qtd_teorica_total", "string"), ("qtd_redutor", "double", "qtd_redutor", "string"), ("dt_extracao", "string", "dt_extracao", "timestamp")], transformation_ctx="ChangeSchema_node1736384274380")

# Script generated for node Aggregate
Aggregate_node1736383572625 = sparkAggregate(glueContext, parentFrame = ChangeSchema_node1736384274380, groups = ["NomeAcao", "dt_referencia_carteira"], aggs = [["val_participacao_setor", "sum"], ["cod_empresa", "countDistinct"]], transformation_ctx = "Aggregate_node1736383572625")

# Script generated for node Select Fields
SelectFields_node1736449430913 = SelectFields.apply(frame=ChangeSchema_node1736384274380, paths=["QtdTeoricaTotal", "NomeAcao", "DataReferenciaCarteira", "dt_referencia_carteira", "qtd_teorica_total"], transformation_ctx="SelectFields_node1736449430913")

# Script generated for node Renamed keys for Join
RenamedkeysforJoin_node1736819163285 = ApplyMapping.apply(frame=SelectFields_node1736449430913, mappings=[("NomeAcao", "string", "right_NomeAcao", "string"), ("dt_referencia_carteira", "string", "right_dt_referencia_carteira", "string"), ("qtd_teorica_total", "string", "right_qtd_teorica_total", "string")], transformation_ctx="RenamedkeysforJoin_node1736819163285")

# Script generated for node Join
Join_node1736449634945 = Join.apply(frame1=Aggregate_node1736383572625, frame2=RenamedkeysforJoin_node1736819163285, keys1=["NomeAcao", "dt_referencia_carteira"], keys2=["right_NomeAcao", "right_dt_referencia_carteira"], transformation_ctx="Join_node1736449634945")

# Script generated for node Change Schema
ChangeSchema_node1736451072089 = ApplyMapping.apply(frame=Join_node1736449634945, mappings=[("NomeAcao", "string", "NomeAcao", "string"), ("dt_referencia_carteira", "string", "DataReferenciaCarteira", "string"), ("`sum(val_participacao_setor)`", "double", "SomaParticipacaoSetor", "string"), ("`count(cod_empresa)`", "long", "`count(cod_empresa)`", "long"), ("right_qtd_teorica_total", "string", "QtdTeoricaTotal", "string")], transformation_ctx="ChangeSchema_node1736451072089")

# Script generated for node Custom Transform
CustomTransform_node1736384318504 = MyTransform(glueContext, DynamicFrameCollection({"ChangeSchema_node1736451072089": ChangeSchema_node1736451072089}, glueContext))

# Script generated for node Select From Collection
SelectFromCollection_node1736385595940 = SelectFromCollection.apply(dfc=CustomTransform_node1736384318504, key=list(CustomTransform_node1736384318504.keys())[0], transformation_ctx="SelectFromCollection_node1736385595940")

# Script generated for node Amazon S3
AmazonS3_node1736387675044 = glueContext.getSink(path="s3://refined-bovespa/tb_transformacao_dados_bovespa/", connection_type="s3", updateBehavior="UPDATE_IN_DATABASE", partitionKeys=["DataReferenciaCarteira", "NomeAcao"], enableUpdateCatalog=True, transformation_ctx="AmazonS3_node1736387675044")
AmazonS3_node1736387675044.setCatalogInfo(catalogDatabase="default",catalogTableName="tb_transformacao_dados_bovespa")
AmazonS3_node1736387675044.setFormat("glueparquet", compression="snappy")
AmazonS3_node1736387675044.writeFrame(SelectFromCollection_node1736385595940)
job.commit()
```

### **Lambda para Acionar o Glue**
A fun√ß√£o Lambda √© respons√°vel por iniciar o job Glue automaticamente quando novos arquivos s√£o carregados no bucket S3.

Exemplo do c√≥digo da Lambda:
```python
import boto3

glue_job_name = "etl-transformacao-dados-bovespa"
glue_client = boto3.client('glue')

def lambda_handler(event, context):
    try:
        response = glue_client.start_job_run(JobName=glue_job_name)
        print(f"Job {glue_job_name} iniciado com sucesso! JobRunId: {response['JobRunId']}")
    except Exception as e:
        print(f"Erro ao iniciar o Job Glue: {str(e)}")
```

### **Lambda para Extra√ß√£o de Empresas IBOV**
A fun√ß√£o Lambda coleta dados de empresas listadas no √≠ndice IBOV, processa as informa√ß√µes e exporta os dados como arquivos Parquet para o S3 em parti√ß√µes di√°rias.  
Exemplo do c√≥digo da Lambda:
```python
import requests
import pandas as pd
import json
import base64
from constantes import *
from utilidades import MLET3
from numpy import int64, float64
from datetime import datetime
import boto3
import io

def lambda_handler(event, context):
    u = MLET3()

    # Converter payload em base64
    def payload_2_base64(pPayload_Dict) -> base64.b64encode:
        vPayload_Json = json.dumps(pPayload_Dict)
        vPayload_base64 = base64.b64encode(vPayload_Json.encode()).decode()
        return vPayload_base64

    # Requisi√ß√£o para API IBOV
    def empresas_IBOV(pLink_API: str = LINK_API_IBOV, pNumeroPagina: int = 1, pQtd_Itens_Pagina: int = 100, pSegmentoEmpresas: int = SEGMENTO_CONSULTAR_POR_CODIGO) -> tuple:
        # Payload
        vPayload = {
            'language': 'pt-br',
            'pageNumber': pNumeroPagina,
            'pageSize': pQtd_Itens_Pagina,
            'index': 'IBOV',
            'segment': pSegmentoEmpresas
        }
        vLinkAPI = ''.join([pLink_API, payload_2_base64(vPayload)])
        vRequisicao = requests.get(vLinkAPI)
        vCabecalho_Json = vRequisicao.json().get("header", [])
        vEmpresas_Json = vRequisicao.json().get("results", [])
        vTotalPaginas = vRequisicao.json()['page']['totalPages']
        return (vTotalPaginas, vCabecalho_Json, vEmpresas_Json)

    # Converter DataFrame para parquet em mem√≥ria
    def converterDataFrame_Parquet_Memoria(pDataFrame: pd.DataFrame) -> io.BytesIO:
        vDataFrame_Buffer = io.BytesIO()
        pDataFrame.to_parquet(vDataFrame_Buffer, index=False)
        vDataFrame_Buffer.seek(0)
        return vDataFrame_Buffer

    # Exportar para bucket S3
    def exportarParaBucketS3(pDataFrameIO: io.BytesIO, pEndpoint: str, pId: str, pSenha: str, pRegiao: str, pBucket: str, pNomeArquivoNoBucket: str, pLocalExecucao: int = 0):

        # Quando pLocalExecucao = 0, LocalStack
        # Quando pLocalExecucao = 1, AWS Lambda
        if pLocalExecucao == 0:
            vCliente_S3 = boto3.client('s3', endpoint_url=pEndpoint, aws_access_key_id=pId, aws_secret_access_key=pSenha, region_name=pRegiao)
        else:
            vCliente_S3 = boto3.client('s3')

        vCliente_S3.upload_fileobj(pDataFrameIO, pBucket, pNomeArquivoNoBucket)


    # Pegar empresas via API
    vListaEmpresas_Json = []
    vContadorPagina = 1
    vTotalPaginas = 1

    while vContadorPagina <= vTotalPaginas:
        vResultadoEmpresas = empresas_IBOV(pNumeroPagina=vContadorPagina, pQtd_Itens_Pagina=100, pSegmentoEmpresas=SEGMENTO_CONSULTAR_POR_SETOR_ATUACAO)
        vTotalPaginas = vResultadoEmpresas[0]
        vCabecalhoEmpresas_Json = vResultadoEmpresas[1]
        vListaEmpresas_Json.extend(vResultadoEmpresas[2])
        vContadorPagina += 1

    # DataFrame
    if vListaEmpresas_Json:
        # Converter a lista em DataFrame
        vBase = pd.DataFrame(vListaEmpresas_Json)

        # Adicionar valores do header
        vBase['dt_referencia_carteira'] = int(pd.to_datetime(vCabecalhoEmpresas_Json['date'], format='%d/%m/%y').strftime('%Y%m%d'))
        vBase['qtd_teorica_total'] = float64(vCabecalhoEmpresas_Json['theoricalQty'].replace('.', '').replace(',', '.'))
        vBase['qtd_redutor'] = float64(vCabecalhoEmpresas_Json['reductor'].replace('.', '').replace(',', '.'))
        vBase['dt_extracao'] = datetime.now()

        # Renomear colunas e formatar
        vBase.rename(columns={
            'segment': 'nme_setor',
            'cod': 'cod_empresa',
            'asset': 'nme_acao',
            'type': 'dsc_tipo',
            'part': 'val_participacao_setor',
            'partAcum': 'val_participacao_acumulada_setor',
            'theoricalQty': 'qtd_teorica'
        }, inplace=True)
        vBase.columns = [u.normalizarTexto(i) for i in vBase.columns]

        # Converter colunas
        vBase[['VAL_PARTICIPACAO_SETOR', 'VAL_PARTICIPACAO_ACUMULADA_SETOR', 'QTD_TEORICA']] = vBase[['VAL_PARTICIPACAO_SETOR', 'VAL_PARTICIPACAO_ACUMULADA_SETOR', 'QTD_TEORICA']].apply(lambda x: x.str.replace('.', '').str.replace(',', '.')).astype(float64)

        # Remover excesso de espa√ßos
        vBase = vBase.apply(lambda x: x.str.replace(r'\s+', ' ', regex=True).str.strip() if x.dtypes == 'object' else x)

        # Extraindo ano, m√™s e dia da data de extra√ß√£o
        ano = vBase['DT_EXTRACAO'].max().year
        mes = vBase['DT_EXTRACAO'].max().month
        dia = vBase['DT_EXTRACAO'].max().day

        # Definindo o caminho particionado no S3
        vNomeArquivo = f"{ano}/{mes:02}/{dia:02}/Empresas_IBOV_{vBase['DT_REFERENCIA_CARTEIRA'].max()}_{vBase['DT_EXTRACAO'].max().strftime('%Y%m%d')}.parquet"

        # Exportar para o S3
        exportarParaBucketS3(
            pDataFrameIO = converterDataFrame_Parquet_Memoria(vBase),
            pEndpoint = r'http://localhost:4566',
            pId = None,
            pSenha = None,
            pBucket = 'stage-dados-brutos',
            pRegiao = 'us-east-1',
            pNomeArquivoNoBucket = vNomeArquivo,
            pLocalExecucao = 1,
            pDiretorio= vBase['DT_EXTRACAO'].max().strftime('ano=%Y/mes=%m/dia=%d')
            )
        
    return {"statusCode": 200, "body": f"Arquivo '{vNomeArquivo}' enviado com sucesso!"}
```
