# Tech Challenge 3MLET

## Equipe
**Membros:**  
- Kleryton de Souza Maria  
- Lucas Paim de Paula  
- Maiara Giavoni  
- Rafael Tafelli dos Santos 

## **Tech Challenge - Fase 3**

Nesta fase, foi desenvolvido um modelo de **Machine Learning** para previsÃ£o de dados do mercado financeiro, utilizando dados extraÃ­dos e armazenados na **Fase 2**. O objetivo foi aprimorar as anÃ¡lises e fornecer uma interface interativa para interpretaÃ§Ã£o dos resultados.

### **Objetivo do Projeto**

Criar um modelo preditivo para identificar tendÃªncias no mercado de aÃ§Ãµes, utilizando tÃ©cnicas avanÃ§adas de Machine Learning. O pipeline completo inclui coleta de dados, processamento, modelagem e visualizaÃ§Ã£o interativa.

### **Atendimento aos Requisitos**

A seguir, indicamos onde cada requisito do **Tech Challenge - Fase 3** foi atendido:

1. **API para coleta de dados em tempo real e armazenamento em Data Lake:**  
   - Reaproveitamos a **API desenvolvida na Fase 2** para coletar dados do mercado financeiro e armazenÃ¡-los no **AWS S3**.  
   - **Mais detalhes:** SeÃ§Ã£o **"Tech Challenge - Fase 2"**.

2. **ConstruÃ§Ã£o de um modelo de Machine Learning:**  
   - Desenvolvemos um modelo utilizando **Random Forest Regressor** para prever tendÃªncias do mercado financeiro.  
   - **Mais detalhes:** SeÃ§Ã£o **"ConstruÃ§Ã£o do Modelo de Machine Learning"**.

3. **CÃ³digo no GitHub com documentaÃ§Ã£o completa:**  
   - Todo o cÃ³digo estÃ¡ disponÃ­vel no repositÃ³rio, junto com este README e demais documentaÃ§Ãµes.  

4. **Storytelling explicando todas as etapas do projeto em vÃ­deo:**  
   - Criamos um vÃ­deo explicativo abordando desde a coleta de dados atÃ© a previsÃ£o dos resultados.  

5. **Modelo produtivo alimentando uma aplicaÃ§Ã£o ou dashboard:**  
   - Implementamos um **dashboard interativo** que apresenta os resultados das previsÃµes do modelo, permitindo anÃ¡lise dinÃ¢mica.  
   - **Mais detalhes:** SeÃ§Ã£o **"Dashboard Interativo"**.

### **Principais EntregÃ¡veis:**

- **Modelo de Machine Learning** baseado em **Random Forest Regressor**.
- **API de coleta de dados**, reaproveitada da **Fase 2**.
- **Armazenamento no AWS S3**, garantindo escalabilidade.
- **Dashboard interativo** para anÃ¡lise das previsÃµes.
- **Storytelling do projeto**, documentado em vÃ­deo.

### **ConstruÃ§Ã£o do Modelo de Machine Learning**

#### **PrÃ©-processamento dos Dados:**
- Tratamento de valores nulos e outliers.
- NormalizaÃ§Ã£o e padronizaÃ§Ã£o das variÃ¡veis.
- SeleÃ§Ã£o das principais caracterÃ­sticas.
- ConversÃ£o de formatos para compatibilidade com o modelo.

#### **DivisÃ£o dos Dados:**
- SeparaÃ§Ã£o em **80% treino** e **20% teste** para garantir uma avaliaÃ§Ã£o justa do modelo.

#### **Escolha do Algoritmo:**
- **Random Forest Regressor** foi escolhido por sua robustez, capacidade de generalizaÃ§Ã£o e eficiÃªncia na modelagem de sÃ©ries temporais e dados financeiros.

#### **Treinamento do Modelo:**
- Ajuste de hiperparÃ¢metros, incluindo:
  - NÃºmero de trees na Random Forest.
  - Profundidade mÃ¡xima das trees.
  - CritÃ©rio de divisÃ£o das trees.
- ValidaÃ§Ã£o cruzada para evitar overfitting e melhorar a generalizaÃ§Ã£o do modelo.
- Ajuste de pesos para lidar com variÃ¡veis de maior impacto no mercado.

#### **AvaliaÃ§Ã£o do Modelo:**
- **MAE (Erro Absoluto MÃ©dio):** 0.0417
- **MSE (Erro QuadrÃ¡tico MÃ©dio):** 0.0101
- **RMSE (Raiz do Erro QuadrÃ¡tico MÃ©dio):** 0.1006

Esses resultados indicam que o modelo apresenta um erro baixo e previsÃµes confiÃ¡veis, permitindo uma boa adaptaÃ§Ã£o Ã s variaÃ§Ãµes do mercado.

#### **ExportaÃ§Ã£o do Modelo:**
- O modelo foi salvo no formato serializado para futuras execuÃ§Ãµes e otimizaÃ§Ãµes.
- ImplementaÃ§Ã£o de um pipeline automatizado para reentrenamento conforme novos dados forem incorporados.
- IntegraÃ§Ã£o direta com o **dashboard interativo** para anÃ¡lise contÃ­nua dos resultados.

### **Dashboard Interativo**

O cÃ³digo do projeto inclui a implementaÃ§Ã£o de um dashboard interativo para visualizar as previsÃµes geradas pelo modelo. Esse dashboard permite a anÃ¡lise detalhada das tendÃªncias do mercado financeiro e a comparaÃ§Ã£o entre os valores previstos e os valores reais.

---

## **ConclusÃ£o sobre o Modelo**

O modelo desenvolvido demonstrou **boa capacidade preditiva**, com baixos Ã­ndices de erro e uma abordagem escalÃ¡vel para anÃ¡lise financeira. A combinaÃ§Ã£o de **Random Forest Regressor** com um pipeline estruturado permitiu previsÃµes confiÃ¡veis, ajudando na tomada de decisÃ£o no mercado financeiro.

Pontos-chave do modelo:
- **Baixo erro mÃ©dio**, indicando alta precisÃ£o nas previsÃµes.
- **AdaptaÃ§Ã£o Ã s flutuaÃ§Ãµes do mercado**, garantindo previsÃµes consistentes.
- **Facilidade de reentrenamento**, permitindo melhoria contÃ­nua com novos dados.
- **IntegraÃ§Ã£o com dashboard**, tornando os insights acessÃ­veis e fÃ¡ceis de interpretar.

Com essa abordagem, garantimos uma soluÃ§Ã£o **precisa, escalÃ¡vel e eficiente**, agregando valor real Ã  anÃ¡lise do mercado financeiro.

ðŸ“Œ **RepositÃ³rio:** [GitHub do Projeto](https://github.com/maiagia/dados-bovespa)  
ðŸ“½ï¸ **VÃ­deo Explicativo:** (inserir link do vÃ­deo)



## Tech Challenge - Fase 2
https://github.com/user-attachments/assets/d8f6d0f6-3d8c-4ed4-9d62-667b1078d489

## DescriÃ§Ã£o do Projeto
Este projeto implementa um pipeline de dados completo para extrair, processar e analisar dados do pregÃ£o da B3 utilizando serviÃ§os da AWS, como **S3**, **Glue**, **Lambda** e **Athena**. O objetivo Ã© automatizar a ingestÃ£o, transformaÃ§Ã£o e anÃ¡lise de dados histÃ³ricos do mercado de aÃ§Ãµes da B3.
---

## Arquitetura
A arquitetura do pipeline segue o seguinte fluxo:

![401786235-579b8781-1676-4acb-b018-dd955ae0e8aa](https://github.com/user-attachments/assets/f5b55231-d0c7-4a10-be8f-6e64c7d82033)


1. **Scrap de Dados (Lambda)**: A funÃ§Ã£o Lambda extrai dados do site da B3 e os envia para o bucket S3 de dados brutos.  
2. **Bucket S3 (Dados Brutos)**: Armazena os dados brutos em formato parquet, particionados diariamente.  
3. **Trigger Lambda**: Quando novos arquivos sÃ£o carregados no S3, uma trigger Lambda inicia o job Glue.  
4. **ETL no Glue**: O job Glue realiza transformaÃ§Ãµes nos dados, como:
   - Agrupamento, sumarizaÃ§Ã£o e contagem.
   - RenomeaÃ§Ã£o de colunas.
   - CÃ¡lculo da diferenÃ§a entre datas.
5. **Bucket S3 (Dados Refinados)**: Os dados transformados sÃ£o salvos em formato **Parquet**, particionados por data e nome da aÃ§Ã£o.
   * O nome do bucket deve ser Ãºnico na aws devido a isso, foi nomeado de refined-bovespa.  
7. **Glue Catalog**: Os metadados dos dados refinados sÃ£o automaticamente catalogados no Glue, criando uma tabela no banco de dados padrÃ£o.  
8. **Athena**: Permite consultas SQL interativas e anÃ¡lises dos dados catalogados.
9. **Notebook**: Oferece uma interface interativa para executar consultas SQL diretamente nos dados catalogados no Glue, permitindo anÃ¡lises exploratÃ³rias, visualizaÃ§Ã£o de resultados e criaÃ§Ã£o de relatÃ³rios de forma prÃ¡tica, sem necessidade de ferramentas externas. 

---

## Requisitos Atendidos

### 1. **Scrap de Dados do Site da B3**
- A funÃ§Ã£o Lambda extrai os dados diretamente do site da B3 e os envia ao bucket S3 de dados brutos. `Lambda request-ibov`

### 2. **IngestÃ£o de Dados no S3**
- Os dados sÃ£o armazenados em S3 no formato **Parquet**, organizados em partiÃ§Ãµes diÃ¡rias.

### 3. **Trigger Lambda**
- A Lambda monitora o bucket de dados brutos e aciona o job Glue automaticamente quando novos arquivos sÃ£o carregados. `Lambda triggerGlueBovespa`

### 4. **TransformaÃ§Ãµes no Glue**
`etl-transformacao-dados-bovespa`.

#### **Agrupamento numÃ©rico, sumarizaÃ§Ã£o e contagem (5A):**
- **NÃ³ usado:** `Aggregate_node1736383572625`.
- **OperaÃ§Ãµes realizadas:**
  - **SumarizaÃ§Ã£o:** Soma da coluna `val_participacao_setor`.
  - **Contagem:** Contagem distinta de `cod_empresa`.

#### **RenomeaÃ§Ã£o de colunas (5B):**
- **NÃ³ usado:** `ChangeSchema_node1736384274380`.
- **Colunas renomeadas:**
  - `nme_acao` â†’ `NomeAcao`.
  - `dt_referencia_carteira` â†’ `DataReferenciaCarteira`.

#### **CÃ¡lculo com campos de data (5C):**
- **FunÃ§Ã£o personalizada:** `MyTransform`.
- **DescriÃ§Ã£o:** Calcula a diferenÃ§a de dias entre a data atual (`current_date()`) e a coluna `DataReferenciaCarteira`. O resultado Ã© armazenado em uma nova coluna, `DiferencaDias`.

### 5. **Salvar os Dados Refinados no S3**
- **Local de salvamento:** `s3://refined-bovespa/tb_transformacao_dados_bovespa/`.
- **PartiÃ§Ãµes:** `DataReferenciaCarteira` e `NomeAcao`.

### 6. **Catalogar Dados no Glue Catalog**
- Os dados transformados sÃ£o automaticamente catalogados no Glue Catalog, permitindo consultas no Athena.

### 7. **Disponibilizar Dados no Athena**
- Como os dados sÃ£o catalogados automaticamente, eles ficam disponÃ­veis para consultas SQL no Athena.

### 8. **CriaÃ§Ã£o do Notebook no Athena**
- No notebook, inclua o cÃ³digo necessÃ¡rio para manipulaÃ§Ã£o de dados e anÃ¡lises adicionais.

---

## Etapas do Pipeline

### **Job ETL no Glue**
O job foi desenvolvido no AWS Glue Studio e realiza as seguintes operaÃ§Ãµes:
- **Leitura dos Dados**: Importa os dados brutos do S3 no formato **Parquet**.
- **TransformaÃ§Ã£o dos Dados**:
  - Renomeia colunas para adequaÃ§Ã£o do esquema.
  - Agrega e calcula mÃ©tricas numÃ©ricas.
  - Calcula diferenÃ§as entre datas.
- **Escrita dos Dados**: Salva os dados transformados em formato **Parquet** no S3, particionados por data e nome da aÃ§Ã£o.
- **CatalogaÃ§Ã£o**: Atualiza automaticamente o Glue Catalog com os metadados dos dados refinados.
Exemplo do cÃ³digo do ETL:
```python
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrameCollection
from awsglue.dynamicframe import DynamicFrame
from pyspark.sql import functions as SqlFuncs

# Script generated for node Custom Transform
def MyTransform(glueContext, dfc) -> DynamicFrameCollection:
    # Importando as bibliotecas necessÃ¡rias dentro da funÃ§Ã£o
    from pyspark.sql.functions import col, datediff, to_date, current_date
    from awsglue.dynamicframe import DynamicFrame

    # Configura o Spark para usar o parser de data legado
    glueContext.spark_session.conf.set("spark.sql.legacy.timeParserPolicy", "LEGACY")

    # Acessa o DynamicFrame especÃ­fico dentro do DynamicFrameCollection
    dynamic_frame = dfc["ChangeSchema_node1736451072089"]

    # Converte o DynamicFrame para DataFrame para usar as funÃ§Ãµes Spark
    df = dynamic_frame.toDF()

    # Converte as colunas para 'timestamp' (se necessÃ¡rio)
    df = df.withColumn(
        "DataReferenciaCarteira", 
        to_date(col("DataReferenciaCarteira"), "yyyyMMdd")  # Converte 'DataReferenciaCarteira' para tipo Date
    )

    # Calcula a diferenÃ§a entre a data atual e a coluna 'DataReferenciaCarteira'
    df = df.withColumn(
        "DiferencaDias", 
        datediff(current_date(), col("DataReferenciaCarteira"))
    )

    # Substitui valores nulos por 0
    df = df.fillna({"DiferencaDias": 0})

    # Reorganiza as colunas para a ordem desejada
    df = df.select(
        "NomeAcao", 
        "DataReferenciaCarteira", 
        "QtdTeoricaTotal", 
        "DiferencaDias", 
        "SomaParticipacaoSetor", 
        "ContagemEmpresas"
    )

    # Converte o DataFrame de volta para DynamicFrame
    transformed_dynamic_frame = DynamicFrame.fromDF(df, glueContext, "transformed_dynamic_frame")

    # Retorna o DynamicFrame dentro de um DynamicFrameCollection
    return DynamicFrameCollection({"transformed_dynamic_frame": transformed_dynamic_frame}, glueContext)
def sparkAggregate(glueContext, parentFrame, groups, aggs, transformation_ctx) -> DynamicFrame:
    aggsFuncs = []
    for column, func in aggs:
        aggsFuncs.append(getattr(SqlFuncs, func)(column))
    result = parentFrame.toDF().groupBy(*groups).agg(*aggsFuncs) if len(groups) > 0 else parentFrame.toDF().agg(*aggsFuncs)
    return DynamicFrame.fromDF(result, glueContext, transformation_ctx)

args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Script generated for node Amazon S3
AmazonS3_node1736304019816 = glueContext.create_dynamic_frame.from_options(format_options={}, connection_type="s3", format="parquet", connection_options={"paths": ["s3://stage-dados-brutos/ano=2025/"], "recurse": True}, transformation_ctx="AmazonS3_node1736304019816")

# Script generated for node Change Schema
ChangeSchema_node1736384274380 = ApplyMapping.apply(frame=AmazonS3_node1736304019816, mappings=[("nme_setor", "string", "nme_setor", "string"), ("cod_empresa", "string", "cod_empresa", "string"), ("nme_acao", "string", "NomeAcao", "string"), ("dsc_tipo", "string", "dsc_tipo", "string"), ("val_participacao_setor", "double", "val_participacao_setor", "double"), ("val_participacao_acumulada_setor", "double", "val_participacao_acumulada_setor", "string"), ("qtd_teorica", "double", "qtd_teorica", "string"), ("dt_referencia_carteira", "bigint", "dt_referencia_carteira", "string"), ("qtd_teorica_total", "double", "qtd_teorica_total", "string"), ("qtd_redutor", "double", "qtd_redutor", "string"), ("dt_extracao", "string", "dt_extracao", "timestamp")], transformation_ctx="ChangeSchema_node1736384274380")

# Script generated for node Aggregate
Aggregate_node1736383572625 = sparkAggregate(glueContext, parentFrame = ChangeSchema_node1736384274380, groups = ["NomeAcao", "dt_referencia_carteira"], aggs = [["val_participacao_setor", "sum"], ["cod_empresa", "countDistinct"]], transformation_ctx = "Aggregate_node1736383572625")

# Script generated for node Select Fields
SelectFields_node1736449430913 = SelectFields.apply(frame=ChangeSchema_node1736384274380, paths=["QtdTeoricaTotal", "NomeAcao", "DataReferenciaCarteira", "dt_referencia_carteira", "qtd_teorica_total"], transformation_ctx="SelectFields_node1736449430913")

# Script generated for node Renamed keys for Join
RenamedkeysforJoin_node1736819163285 = ApplyMapping.apply(frame=SelectFields_node1736449430913, mappings=[("NomeAcao", "string", "right_NomeAcao", "string"), ("dt_referencia_carteira", "string", "right_dt_referencia_carteira", "string"), ("qtd_teorica_total", "string", "right_qtd_teorica_total", "string")], transformation_ctx="RenamedkeysforJoin_node1736819163285")

# Script generated for node Join
Join_node1736449634945 = Join.apply(frame1=Aggregate_node1736383572625, frame2=RenamedkeysforJoin_node1736819163285, keys1=["NomeAcao", "dt_referencia_carteira"], keys2=["right_NomeAcao", "right_dt_referencia_carteira"], transformation_ctx="Join_node1736449634945")

# Script generated for node Change Schema
ChangeSchema_node1736451072089 = ApplyMapping.apply(frame=Join_node1736449634945, mappings=[("NomeAcao", "string", "NomeAcao", "string"), ("dt_referencia_carteira", "string", "DataReferenciaCarteira", "string"), ("`sum(val_participacao_setor)`", "double", "SomaParticipacaoSetor", "string"), ("`count(cod_empresa)`", "long", "`count(cod_empresa)`", "long"), ("right_qtd_teorica_total", "string", "QtdTeoricaTotal", "string")], transformation_ctx="ChangeSchema_node1736451072089")

# Script generated for node Custom Transform
CustomTransform_node1736384318504 = MyTransform(glueContext, DynamicFrameCollection({"ChangeSchema_node1736451072089": ChangeSchema_node1736451072089}, glueContext))

# Script generated for node Select From Collection
SelectFromCollection_node1736385595940 = SelectFromCollection.apply(dfc=CustomTransform_node1736384318504, key=list(CustomTransform_node1736384318504.keys())[0], transformation_ctx="SelectFromCollection_node1736385595940")

# Script generated for node Amazon S3
AmazonS3_node1736387675044 = glueContext.getSink(path="s3://refined-bovespa/tb_transformacao_dados_bovespa/", connection_type="s3", updateBehavior="UPDATE_IN_DATABASE", partitionKeys=["DataReferenciaCarteira", "NomeAcao"], enableUpdateCatalog=True, transformation_ctx="AmazonS3_node1736387675044")
AmazonS3_node1736387675044.setCatalogInfo(catalogDatabase="default",catalogTableName="tb_transformacao_dados_bovespa")
AmazonS3_node1736387675044.setFormat("glueparquet", compression="snappy")
AmazonS3_node1736387675044.writeFrame(SelectFromCollection_node1736385595940)
job.commit()
```

### **Lambda para Acionar o Glue**
A funÃ§Ã£o Lambda Ã© responsÃ¡vel por iniciar o job Glue automaticamente quando novos arquivos sÃ£o carregados no bucket S3.

Exemplo do cÃ³digo da Lambda:
```python
import boto3

glue_job_name = "etl-transformacao-dados-bovespa"
glue_client = boto3.client('glue')

def lambda_handler(event, context):
    try:
        response = glue_client.start_job_run(JobName=glue_job_name)
        print(f"Job {glue_job_name} iniciado com sucesso! JobRunId: {response['JobRunId']}")
    except Exception as e:
        print(f"Erro ao iniciar o Job Glue: {str(e)}")
```

### **Lambda para ExtraÃ§Ã£o de Empresas IBOV**
A funÃ§Ã£o Lambda coleta dados de empresas listadas no Ã­ndice IBOV, processa as informaÃ§Ãµes e exporta os dados como arquivos Parquet para o S3 em partiÃ§Ãµes diÃ¡rias.  
Exemplo do cÃ³digo da Lambda:
```python
import requests
import pandas as pd
import json
import base64
from constantes import *
from utilidades import MLET3
from numpy import int64, float64
from datetime import datetime
import boto3
import io

def lambda_handler(event, context):
    u = MLET3()

    # Converter payload em base64
    def payload_2_base64(pPayload_Dict) -> base64.b64encode:
        vPayload_Json = json.dumps(pPayload_Dict)
        vPayload_base64 = base64.b64encode(vPayload_Json.encode()).decode()
        return vPayload_base64

    # RequisiÃ§Ã£o para API IBOV
    def empresas_IBOV(pLink_API: str = LINK_API_IBOV, pNumeroPagina: int = 1, pQtd_Itens_Pagina: int = 100, pSegmentoEmpresas: int = SEGMENTO_CONSULTAR_POR_CODIGO) -> tuple:
        # Payload
        vPayload = {
            'language': 'pt-br',
            'pageNumber': pNumeroPagina,
            'pageSize': pQtd_Itens_Pagina,
            'index': 'IBOV',
            'segment': pSegmentoEmpresas
        }
        vLinkAPI = ''.join([pLink_API, payload_2_base64(vPayload)])
        vRequisicao = requests.get(vLinkAPI)
        vCabecalho_Json = vRequisicao.json().get("header", [])
        vEmpresas_Json = vRequisicao.json().get("results", [])
        vTotalPaginas = vRequisicao.json()['page']['totalPages']
        return (vTotalPaginas, vCabecalho_Json, vEmpresas_Json)

    # Converter DataFrame para parquet em memÃ³ria
    def converterDataFrame_Parquet_Memoria(pDataFrame: pd.DataFrame) -> io.BytesIO:
        vDataFrame_Buffer = io.BytesIO()
        pDataFrame.to_parquet(vDataFrame_Buffer, index=False)
        vDataFrame_Buffer.seek(0)
        return vDataFrame_Buffer

    # Exportar para bucket S3
    def exportarParaBucketS3(pDataFrameIO: io.BytesIO, pEndpoint: str, pId: str, pSenha: str, pRegiao: str, pBucket: str, pNomeArquivoNoBucket: str, pLocalExecucao: int = 0):

        # Quando pLocalExecucao = 0, LocalStack
        # Quando pLocalExecucao = 1, AWS Lambda
        if pLocalExecucao == 0:
            vCliente_S3 = boto3.client('s3', endpoint_url=pEndpoint, aws_access_key_id=pId, aws_secret_access_key=pSenha, region_name=pRegiao)
        else:
            vCliente_S3 = boto3.client('s3')

        vCliente_S3.upload_fileobj(pDataFrameIO, pBucket, pNomeArquivoNoBucket)


    # Pegar empresas via API
    vListaEmpresas_Json = []
    vContadorPagina = 1
    vTotalPaginas = 1

    while vContadorPagina <= vTotalPaginas:
        vResultadoEmpresas = empresas_IBOV(pNumeroPagina=vContadorPagina, pQtd_Itens_Pagina=100, pSegmentoEmpresas=SEGMENTO_CONSULTAR_POR_SETOR_ATUACAO)
        vTotalPaginas = vResultadoEmpresas[0]
        vCabecalhoEmpresas_Json = vResultadoEmpresas[1]
        vListaEmpresas_Json.extend(vResultadoEmpresas[2])
        vContadorPagina += 1

    # DataFrame
    if vListaEmpresas_Json:
        # Converter a lista em DataFrame
        vBase = pd.DataFrame(vListaEmpresas_Json)

        # Adicionar valores do header
        vBase['dt_referencia_carteira'] = int(pd.to_datetime(vCabecalhoEmpresas_Json['date'], format='%d/%m/%y').strftime('%Y%m%d'))
        vBase['qtd_teorica_total'] = float64(vCabecalhoEmpresas_Json['theoricalQty'].replace('.', '').replace(',', '.'))
        vBase['qtd_redutor'] = float64(vCabecalhoEmpresas_Json['reductor'].replace('.', '').replace(',', '.'))
        vBase['dt_extracao'] = datetime.now()

        # Renomear colunas e formatar
        vBase.rename(columns={
            'segment': 'nme_setor',
            'cod': 'cod_empresa',
            'asset': 'nme_acao',
            'type': 'dsc_tipo',
            'part': 'val_participacao_setor',
            'partAcum': 'val_participacao_acumulada_setor',
            'theoricalQty': 'qtd_teorica'
        }, inplace=True)
        vBase.columns = [u.normalizarTexto(i) for i in vBase.columns]

        # Converter colunas
        vBase[['VAL_PARTICIPACAO_SETOR', 'VAL_PARTICIPACAO_ACUMULADA_SETOR', 'QTD_TEORICA']] = vBase[['VAL_PARTICIPACAO_SETOR', 'VAL_PARTICIPACAO_ACUMULADA_SETOR', 'QTD_TEORICA']].apply(lambda x: x.str.replace('.', '').str.replace(',', '.')).astype(float64)

        # Remover excesso de espaÃ§os
        vBase = vBase.apply(lambda x: x.str.replace(r'\s+', ' ', regex=True).str.strip() if x.dtypes == 'object' else x)

        # Extraindo ano, mÃªs e dia da data de extraÃ§Ã£o
        ano = vBase['DT_EXTRACAO'].max().year
        mes = vBase['DT_EXTRACAO'].max().month
        dia = vBase['DT_EXTRACAO'].max().day

        # Definindo o caminho particionado no S3
        vNomeArquivo = f"{ano}/{mes:02}/{dia:02}/Empresas_IBOV_{vBase['DT_REFERENCIA_CARTEIRA'].max()}_{vBase['DT_EXTRACAO'].max().strftime('%Y%m%d')}.parquet"

        # Exportar para o S3
        exportarParaBucketS3(
            pDataFrameIO = converterDataFrame_Parquet_Memoria(vBase),
            pEndpoint = r'http://localhost:4566',
            pId = None,
            pSenha = None,
            pBucket = 'stage-dados-brutos',
            pRegiao = 'us-east-1',
            pNomeArquivoNoBucket = vNomeArquivo,
            pLocalExecucao = 1,
            pDiretorio= vBase['DT_EXTRACAO'].max().strftime('ano=%Y/mes=%m/dia=%d')
            )
        
    return {"statusCode": 200, "body": f"Arquivo '{vNomeArquivo}' enviado com sucesso!"}
```
